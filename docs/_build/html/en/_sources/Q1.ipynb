{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76744efe-40d3-4dba-a19d-509f10b87525",
   "metadata": {},
   "source": [
    "## Pydap is slow. How can I improve the download time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54302380-c12c-4e52-a95a-c59da34d7d45",
   "metadata": {},
   "source": [
    "There are two stages at which `pydap` downloads content: `a)` during the dataset creation, and `b)` fetching numerical / array data.\n",
    "\n",
    "### <font size=\"5\"><span style='color:#0066cc'> **a) Metadata / dataset creation**<font size=\"3\">\n",
    "```python\n",
    "pyds = open_url(URL, session=my_session, protocol='dap2') # protocol=\"dap4\" \n",
    "```\n",
    "The performace depends on various factors:\n",
    "* **Authentication**. There may be many redirects. If possible, use `token` authentication, as it reduces the amount of redirects.\n",
    "  \n",
    "* **Hierarchical metadata**. Some datasets can contain O(100) of variables, and complex nested `Groups` (`Groups` are part of the `DAP4` protocol), and parsing the metadata to create the dataset can be time-consuming. To reduce the timing, you can use the [Data Request Form](https://www.opendap.org/support/online-help-files/) to construct a `Constraint Expression` that reduces the amount of `Groups` and variables you wish to include in your dataset. This will allow you to discard variables before creating the dataset. The documentation on [Constraint expressions](ConstraintExpressions) has an example demonstrating the use of `CE`s to reduce the size of the dataset before the dataset creation for the Atlas03 experiment.\n",
    "\n",
    "* **Cache the Session**. Starting with `pydap` version `3.5.4`, `pydap` can cache sessions, storing the `dmr` (i.e. the metadata) after the first download, for later use.\n",
    "```{note}\n",
    "`requests-cache` can also recover credentials from the `~/.netrc` file, and handle token authentication.\n",
    "```\n",
    "To cache the session, you can initialize it as follows\n",
    "```python\n",
    "from pydap.net import create_session\n",
    "\n",
    "my_session = create_session(use_cache=True) # False is the default\n",
    "pyds = open_url(URL, session=my_session, protocol='dap4') # protocol=\"dap2\" works too\n",
    "```\n",
    "The documentation section on [Pydap as a Client](PydapAsClient) has a short example showing how to cache the `dmr` during the dataset creation.\n",
    "\n",
    "\n",
    "### <font size=\"5.5\"><span style='color:#0066cc'>**b) Fetching numerical data** <font size=\"3.5\">\n",
    "\n",
    "`pydap` downloads array data in the form of `.dap` (DAP4) or `.dods` (DAP2) when slicing the array. This is, when:\n",
    "```python\n",
    "pyds[\"VarName\"][:] # this will download all the array, a different indexing will only download the subset\n",
    "```\n",
    "or when accessing via `xarray` (with `engine=\"pydap\"`)\n",
    "```python\n",
    "ds['varName'].isel(dim1=slice_dim1, dim2=slice_dim2).data # e.g. ds['Theta'].isel(X=slice(1,10), Y=slice(10, 20)).data\n",
    "```\n",
    "The speed of download can depend on many factors: chunking of the remote dataset, size of download, internet speed, the remote server, etc. We recommend:\n",
    "\n",
    "* **Subset the Variable**. This limits the size of download (specially when remote datasets are a virtual aggregated of many many remote files). Some organizations impose a 2Gb limit on the download. The [PACE Example](notebooks/PACE) illustrates this point. In it, the coords arrays (`lat` and `lon`) are to identify the subset of 2D array of interest. \n",
    "\n",
    "* **Cache the Session** . Same as with the dataset creation, a cached session can also store `.dap`/`.dods` responses. This will also limit the times a (repeated) download is requested to the server. \n",
    "\n",
    "* **Diagnosing**. It is possible that the remote dataset has many small chunks, resulting in very slow performance. This, along with internet connection, are performance problems outside of the scope of `pydap`. A useful diagnose if the issue is withg `pydap` or with the remote server, is to use curl to download the response.\n",
    "```python\n",
    "curl -L -n \"<opendap_url_with_constraint_expression>\" \n",
    "```\n",
    "where `-L` implies following redirects, and `-n` instructs `curl` to recover authentication from the `~/.netrc` file. This last one is only necessary when authentication is required. For example, to download a `.dap` (DAP4) response from a dap4 server (with no authentication required):\n",
    "```python\n",
    "curl -L -o output.dap \"http://test.opendap.org/opendap/data/nc/coads_climatology.nc.dap?dap4.ce=/TIME\"\n",
    "```\n",
    "The following command downloads only the variable `TIME` from [this test](http://test.opendap.org/opendap/data/nc/coads_climatology.nc.dmr) dataset. The download should be very fast. When slicing an array `pydap` does something very similar: downloads a `.dap` response for a single variable, in this case `TIME`. Pydap should not take too much longer that `curl` to download the `.dap` response.\n",
    "\n",
    "* **Check variable sizes and avoid downloading entire arrays of ncml datasets**. `ncml` datasets are a virtual aggregation of a collection of NetCDF files. The `ncml` is great because it provides a single URL endpoint for a single collection, but many users experience long times and downlod errors when requesting to download even a single variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c81801-87bc-4d6b-a6b8-0126c232837c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
