{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ff2edf-9dd2-47e2-943c-3fd616014ce5",
   "metadata": {},
   "source": [
    "# MERRA-2 Data via NASA's OPeNDAP in the Cloud.\n",
    "\n",
    "**Requirements to run this notebook**\n",
    "\n",
    "1. Have an Earth Data Login account\n",
    "2. Preferred method of authentication.\n",
    "\n",
    "**Objectives**\n",
    " \n",
    "Use best practices from OPeNDAP, [pydap](https://pydap.github.io/pydap/), and xarray, to\n",
    "\n",
    "- Discover all OPeNDAP URLs associated with a MERRA-2 collection.\n",
    "- Authenticate via EDL (token based)\n",
    "- Explore MERRA-2 collection and filter variables\n",
    "- Consolidate Metadata at the collection level\n",
    "- Download/stream a subset of interest.\n",
    "\n",
    "\n",
    "\n",
    "`Author`: Miguel Jimenez-Urias, '25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7498eb-4a64-43a5-b915-3ae56ae4d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimenezm/miniforge3/envs/pydap_docs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pydap.net import create_session\n",
    "from pydap.client import get_cmr_urls, consolidate_metadata, open_url\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "import earthaccess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a18d00-4bf2-4b6f-be32-88674f0bc4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xarray version:  2025.9.0\n",
      "pydap version:  3.5.8\n"
     ]
    }
   ],
   "source": [
    "print(\"xarray version: \", xr.__version__)\n",
    "print(\"pydap version: \", pydap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ed83d-a9a6-4360-849f-8b68eff72ef6",
   "metadata": {},
   "source": [
    "### Explore the MERRA-2 Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d4d706-5781-4f48-99b0-af1c16137192",
   "metadata": {},
   "outputs": [],
   "source": [
    "merra2_doi = \"10.5067/VJAFPLI1CSIV\" # available e.g. GES DISC MERRA-2 documentation \n",
    "                                    # https://disc.gsfc.nasa.gov/datasets/M2T1NXSLV_5.12.4/summary?keywords=MERRA-2\n",
    "# One month of data\n",
    "time_range=[dt.datetime(2023, 1, 1), dt.datetime(2023, 2, 28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f8a9ea-d646-4ebe-a13c-f5402bb06648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = get_cmr_urls(doi=merra2_doi,time_range=time_range, limit=100) # you can incread the limit of results\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4a79d-56d1-428d-8313-9532f130c9f7",
   "metadata": {},
   "source": [
    "### Authenticate\n",
    "\n",
    "To hide the abstraction, we will use earthaccess to authenticate, and create cache session to consolidate all metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2358a842-e4f7-459f-83d7-e2fe3826781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy=\"interactive\", persist=True) # you will be promted to add your EDL credentials\n",
    "\n",
    "# pass Token Authorization to a new Session.\n",
    "cache_kwargs={'cache_name':'data/MERRA2'}\n",
    "my_session = create_session(use_cache=True, session=auth.get_session(), cache_kwargs=cache_kwargs)\n",
    "my_session.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7905e3a-3d14-4a2a-8e77-a6fafcf50b9d",
   "metadata": {},
   "source": [
    "### Explore Variables in collection and filter down to keep only desirable ones\n",
    "\n",
    "We do this by specifying the NASA OPeNDAP server to process requests via the DAP4 protocol.\n",
    "\n",
    "There are two ways to do this:\n",
    "\n",
    "- Use `pydap` to inspect the metadata (all variables inside the files, and their description). You can run the following code to list all variable names. \n",
    "\n",
    "```python\n",
    "from pydap.client import open_url\n",
    "ds = open_url(url, protocol='dap4', session=my_session)\n",
    "ds.tree() # this will display the entire tree directory\n",
    "\n",
    "ds[Varname].attributes # will display all the information about Varname in the remote file.\n",
    "```\n",
    "\n",
    "- Use OPeNDAP's Data Request Form visible from the browser. You accomplish this by taking an OPeNDAP URL, adding the `.dmr` extension, and paste that resulting URL into a browser.\n",
    "\n",
    "Either way, you can inspect variable names, their description, without downloading any large arrays. \n",
    "\n",
    "Below, we assume you know which variables you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f118b9c1-9df6-4ae1-ae20-483cc009a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['lon', 'lat', 'time', 'T2M', \"U2M\", \"V2M\"] # variables of interest\n",
    "CE = \"?dap4.ce=\"+ \"/\"+\";/\".join(variables) # Need to add this string as a query expression to the OPeNDAP URL\n",
    "new_urls = [url.replace(\"https\", \"dap4\") + CE for url in urls] # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030d1b5-4f30-47d4-ad4a-98f779011e1b",
   "metadata": {},
   "source": [
    "### Consolidate Metadata\n",
    "\n",
    "Aggregating multiple remote files at the collection level requires persistent internet connection. The pydap backend allows to download and store the metadata required by xarray locally as a sqlite3 database, and this database can be used as session manager (for futher downloads). Creating this databaset can be done once, and reused, version controlled, etc. Reusing this database can cut the time to generate the aggregated dataset view from minutes to seconds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee8f1e-4da3-4363-9089-c71a808f2fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datacube has dimensions ['lat[0:1:360]', 'lon[0:1:575]'] , and concat dim: `['time']`\n"
     ]
    }
   ],
   "source": [
    "consolidate_metadata(new_urls, session=my_session, concat_dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2a672-9acc-49cd-9fdf-413ee139e3dd",
   "metadata": {},
   "source": [
    "### Explore an Individual Remote \n",
    "\n",
    "We create an Xarray dataset for visualization purposes. Lets make a plot of near surface Temperature, at a single time unit.\n",
    "\n",
    "```{note}\n",
    "Each granule has 24 time units. Chunking in time as done below when generating the dataset, will ensure the slice is passed to the server. And so subsetting takes place close to the data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbb4d6-b9e2-4766-90e6-a999d4662ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_dataset(\n",
    "    new_urls[0], \n",
    "    engine='pydap', \n",
    "    session=my_session, \n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12cb39-8084-4733-b994-a9e4055296f4",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "In `Xarray`, when visualizing data, data gets downloaded. By chunking in `time` when creating the dataset, we ensure in the moment of visualizing a single time-unit in xarray, the time slice is passed to the server. This has the effect of reducing the amount of data downloaded, and faster transfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78169622-c39a-4a73-a8e7-9a73b6488363",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ds['T2M'].isel(time=0).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0fefc-b68c-40f0-ae9e-887afab0fa3f",
   "metadata": {},
   "source": [
    "### Subset the Aggregated Dataset, while resampling in time \n",
    "\n",
    "- We select data relevant to the Southern Hemisphere, in the area near South America.\n",
    "- Store data in daily averages (as opposed to hourly data).\n",
    "\n",
    "```{note}\n",
    "To accomplish this, we need to identify the slice in index space, and `chunk the dataset` before streaming. Doing so will ensure spatial subsetting is done on the server side (OPeNDAP) close to the data, while `Xarray` performs the daily average.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5ef38-e761-4343-849f-d556dc7a1763",
   "metadata": {},
   "source": [
    "### Create Dataset Aggregation, Chunking in Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4072d-5b14-42e3-bfaf-0140cd939cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# spatial subset around South America\n",
    "lat, lon = ds['lat'].data, ds['lon'].data\n",
    "\n",
    "minLon, maxLon = -100, 50 \n",
    "iLon = np.where((lon>minLon)&(lon < maxLon))[0]\n",
    "iLat= np.where(lat < 0)[0]\n",
    "\n",
    "# Make sure subset is done by server\n",
    "expected_download = {'lon':len(iLon), 'lat': len(iLat)}\n",
    "\n",
    "ds = xr.open_mfdataset(\n",
    "    new_urls,\n",
    "    engine='pydap', \n",
    "    session=my_session,\n",
    "    concat_dim='time',\n",
    "    combine='nested',\n",
    "    parallel=True,\n",
    "    chunks=expected_download, \n",
    ")\n",
    "\n",
    "## now subset the Xarray Dataset and rechunk so it is a single chunk\n",
    "ds = ds.isel(lon=slice(iLon[0], iLon[-1]+1), lat=slice(iLat[0], iLat[-1]+1))\n",
    "\n",
    "# take daily average and store locally\n",
    "nds = ds.resample(time=\"1D\").mean()\n",
    "nds.to_netcdf(\"data/Merra2_subset.nc4\", mode='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd7ac5-da68-4525-89f1-377b07c6a73e",
   "metadata": {},
   "source": [
    "### Lets look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05552bb-c16b-45c2-a9d0-d69caf1ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = xr.open_dataset(\"data/Merra2_subset.nc4\")\n",
    "mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80400196-c036-4636-a398-45e384ac92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "mds['T2M'].isel(time=0).plot();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
