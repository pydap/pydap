{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab1781b-9318-4fda-a523-71a60f87aa44",
   "metadata": {},
   "source": [
    "# PACE Chlorophyll A data over a region in the NorthAtlantic Ocean\n",
    "\n",
    "This notebook demonstrates access and subsetting to PACE Ocean Color Data. Broad information about the dataset can be found on the PACE website (see [here](https://oceandata.sci.gsfc.nasa.gov))\n",
    "\n",
    "**Requirements to run this notebook**\n",
    "1. Have an Earth Data Login account\n",
    "2. Have a Bearer Token.\n",
    "3. Knowledge of a collection concept ID (or DOI) of a dataset.\n",
    "4. Internet connection.\n",
    "\n",
    "\n",
    "**Objectives**\n",
    " \n",
    "Use [pydap](https://pydap.github.io/pydap/) to demonstrate\n",
    "\n",
    "- Discovery of OPeNDAP granules from a collection of interest, further filtering with a time range.\n",
    "- Use of `tokens` to authenticate (from earthaccess).\n",
    "- Accessing Level 3 `PACE`, and identify an area of interest.\n",
    "- Subset and enable the `Constraint Expression` to make its way to the OPeNDAP server.\n",
    "- Store the subset locally.\n",
    "\n",
    "`Author`: Miguel Jimenez-Urias, '25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa31dfb-2d83-4e74-b672-57a7c1da0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydap.client import get_cmr_urls, consolidate_metadata, open_url\n",
    "from pydap.net import create_session\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155ad0f-e4aa-410f-aafb-3d9abd3faa27",
   "metadata": {},
   "source": [
    "### Use a Session as a Metadata Database and Authentication\n",
    "\n",
    "Pydap can make use of a `requests_cache.CachedSession` object, which can be used as\n",
    "\n",
    "- Authentication (via netrc or token).\n",
    "- Database Management (sqlite3 as backend). Only caches necessary data.\n",
    "- Stream data.\n",
    "\n",
    "Below we initialize the session, inheriting auth credentials from earthaccess, and point to local database `PACE.sqlite` store in the `/data` folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fed4ae-c1af-406a-bf2b-aa5038d6c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy=\"interactive\", persist=True)\n",
    "session = create_session(use_cache=True, session=earthaccess.get_requests_https_session(), cache_kwargs={'cache_name': 'data/PACE'})\n",
    "session.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a8d5-ad02-4200-970c-8336d4ae08ab",
   "metadata": {},
   "source": [
    "### Query CMR for OPeNDAP URLs\n",
    "\n",
    "We query NASA's CMR for all opendap urls associated with 4km, daily, Level 3 Gridded Chlorophyll A, version 3.1. For this, you can use Earthdata Search to identify the Concept Collection ID. We focus for 2 months of data this year.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add5aa3-35b8-4c4b-9e12-7f5f84f5ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACE_ccid = \"C3620140255-OB_CLOUD\" # version 3.1\n",
    "time_range=[dt.datetime(2025, 6, 1), dt.datetime(2025, 8, 1)]\n",
    "urls = get_cmr_urls(ccid=PACE_ccid, limit=1000, time_range=time_range) # limit by default = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf53c4d-c7d6-402a-9fce-6f9fd183ce8a",
   "metadata": {},
   "source": [
    "\n",
    "```{note}\n",
    "This collection had two version of the same data. We are interested in the `4km` resolution. One way to be certain that any number of opendap urls belong to the same collection, is by inspecting the url strings (unnecessary in most cases)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1aa21e-088a-4728-97e9-2f6c1d212df1",
   "metadata": {},
   "source": [
    "Further, we subset by variable name, retaining only `lat`, `lon`, and `chlor_a` variables. These are the variables of interest, and discarding variables reduces the amount of time it takes for `Xarray` to process the metadata\n",
    "\n",
    "\n",
    "```{note}\n",
    "Xarray has a .drop_vars method, but these variables are dropped AFTER creating the dataset. If a dataset has O(1000) variables, `Xarray` would process the variables first, and then drop them. With OPeNDAP, you can instruct the server a priori which variables Xarray needs to process.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db89be-99e7-4f82-aca2-0e9c9c914c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "CEs = \"?dap4.ce=/lat;/lon;/chlor_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6648b-3c87-4877-8bf6-84ac8c0a7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_urls = [url for url in urls if '4km' in url and \"DAY\" in url]\n",
    "pace_urls = [url.replace(\"https\", \"dap4\") + CEs for url in _urls] # a dap4 schema implies a DAP4 protocol in pydap.\n",
    "len(pace_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb5c27-fd53-43e6-a321-206d1fe88595",
   "metadata": {},
   "outputs": [],
   "source": [
    "pace_urls[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb0fdd-502a-4bc2-9589-f4a58a0ee1f9",
   "metadata": {},
   "source": [
    "### Consolidate metadata aggregation\n",
    "\n",
    "In here, the dimensions and some metadata is eagerly download. This step is not necessary but provides a major boost in the performance when using pydap  as a backend of xarray. \n",
    "\n",
    "The metadata is stored as sqlite, and can persist through sessions.\n",
    "\n",
    "```{note}\n",
    "This collection only defines the time dimension (or coordinate) in the metadata. Because there is no  variable in the remote file associated with the time coordinate, there is no need to define a concatenating dimension (`concat_dim`) when running the code below. Almost always there is a `concat_dim`, but not in this special case. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0f38f-5e59-469f-9476-e972d1448f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "consolidate_metadata(pace_urls, session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c72a71-e5f6-45ae-8c89-2676cef6c061",
   "metadata": {},
   "source": [
    "### Create a single Dataset object\n",
    "\n",
    "We use `Xarray` with `pydap` as the backend. `Pydap` is a backend that is always installed when installing `Xarray`, but not yet the default when working with opendap urls. As a result, we must define it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c124b35-96f1-4e2f-a172-98b62b575691",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    pace_urls, \n",
    "    engine='pydap', \n",
    "    session=session, \n",
    "    parallel=True, \n",
    "    concat_dim='time',  # <------ a time dimension will be created \n",
    "    combine='nested',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc0282-0c00-4fa4-aa8d-b812c22f94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717defc3-2aaa-4e32-9f19-abda7f849c8b",
   "metadata": {},
   "source": [
    "\n",
    "The dataset above represent a metadata representation of the entire data of interest. It is chunked, and only the dimensions lat and lon have been downloaded.\n",
    "\n",
    "```{note}\n",
    "A named dimension `time` was created, along with all remote granules were concatenated.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d1d93-0f01-494c-b86b-24dc53c54ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uncompressed dataset size [GBs]: ', ds.nbytes / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96c84f-954f-4154-a426-d38580cefb19",
   "metadata": {},
   "source": [
    "### Data-proximate subsetting: How to with OPeNDAP, Pydap, and Xarray?\n",
    "\n",
    "\n",
    "While it is possible to download the entire dataset, it is better practice to subset it first! There are many tools to subset data in a data-proximate way, and OPeNDAP is one of them. We need to identify the subset first and pass it to the server.\n",
    "\n",
    "Identifying the subset enables users to potentially construct Constraint Expressions, which are used to instruct the OPeNDAP server the subset of interest. But with Pydap and Xarray, these are tools that interactively construct slices and can request subsets, hiding the abstraction. Below we demonstrate how to pass the subset to the OPeNDAP server so that Xarray does less work.\n",
    "\n",
    "### Identify spatial subset\n",
    "\n",
    "In this case, we are interested in a spatial subset. The data is Level 3 data (gridded) so latitude and longitude are uniform. Moreover, these are 1D, and have already been downloaded into memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7806c-f21f-4a8a-9bd1-33f23005df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = ds['lat'].values, ds['lon'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865db078-4053-47ec-a033-dbbf6c29d489",
   "metadata": {},
   "source": [
    "### Say we define the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d7816-7863-44ba-a1a6-9c6a71b40bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min/max of lon values\n",
    "minLon, maxLon = -96, 10\n",
    "\n",
    "# Min/Max of lat values\n",
    "minLat, maxLat = 6, 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf5f75-a75e-4e84-927c-511e9e183e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "iLon = np.where((lon>minLon)&(lon < maxLon))[0]\n",
    "iLat= np.where((lat>minLat)&(lat < maxLat))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71340e5f-761e-4176-a19d-0eeca7098c7a",
   "metadata": {},
   "source": [
    "So the slices are simply the first and last elements of iLon and iLat!!:\n",
    "\n",
    "```python\n",
    "lon_slice = slice(iLon[0], iLon[-1])\n",
    "lat_slice = slice(iLat[0], iLat[-1])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5c070-ec9f-44cf-82ab-e439dc93e1c5",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Not all array values of `lat/lon` coordinates are monotonic. Always make sure that is the case, even when data is Level 3 or Level 4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e486dc-0d20-4730-a7d2-c5ee5ed3c3ca",
   "metadata": {},
   "source": [
    "### Download only the subset\n",
    "\n",
    "\n",
    "With `Xarray`, subsetting syntax is similar to pandas. For example\n",
    "```\n",
    "ds.isel(lon=lon_slice, lat_slice)\n",
    "```\n",
    "\n",
    "Produces a client-side subset. However\n",
    "\n",
    "```{warning}\n",
    "When using opendap, slice as above and then downloading data will only subset by variable, downloading all of the data in most cases, only to then be further subset by xarray.  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3eb01-5ef4-4146-bbfa-7e030365e4f5",
   "metadata": {},
   "source": [
    "To ensure the slices are sent to the remote server, we must `chunk the dataset` when creating it. This will guarantee that the subset will be mostly done on the server side. You `MUST` choose the chunk size to match the size of the slice, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f44e1-ec4c-411c-a8f5-a57698375e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    pace_urls, \n",
    "    engine='pydap', \n",
    "    session=session, \n",
    "    parallel=True, \n",
    "    concat_dim='time',  # <------ a time dimension will be created \n",
    "    combine='nested',\n",
    "    chunks = {'lon': len(iLon), 'lat':len(iLat)} #  <----------- This instructs the OPeNDAP server to subset in space\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b082a8a-3ce4-4c59-b188-5b1f5f83c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"chlor_a\"] ## inspect the chunk of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254bc2b0-6f86-467c-9feb-ec2323ebbcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04bc3050-cc8f-4ab1-b7fa-4fbbbbbf84b4",
   "metadata": {},
   "source": [
    "We now ensure `Xarray` will treat this subset as an individual chunk (per variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a02d0-9a72-472b-95b2-e2a29c7183fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.isel(lon=slice(iLon[0], iLon[-1]+1), lat=slice(iLat[0], iLat[-1]+1)).chunk({'lon': len(iLon), 'lat':len(iLat)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70431f-9731-4387-a5cf-80f163c08186",
   "metadata": {},
   "source": [
    "### Finally\n",
    "\n",
    "Store data locally, as `NetCDF4`. At this stage:\n",
    "\n",
    "- Data is downloaded (spatially subsetted via `OPeNDAP`)\n",
    "- Data is resampled (via `Xarray`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70b275-590d-43f9-97f7-9e78852e02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.to_netcdf(\"data/pace_subset.nc4\", mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb81c9-5048-4a25-b38e-53461f3b6c27",
   "metadata": {},
   "source": [
    "### We inspect our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7ad3d-58c0-4568-9e20-8cff11cf3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = xr.open_dataset(\"data/pace_subset.nc4\")\n",
    "mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180c469-7472-422b-9eae-d612495ab6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of downloaded subset: \", mds.nbytes/1e9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
