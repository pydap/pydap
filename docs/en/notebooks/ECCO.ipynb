{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba3c59f-0eb1-41a1-a531-4cd99d05d8ce",
   "metadata": {},
   "source": [
    "# ECCOv4 access via NASA's OPeNDAP in the Cloud.\n",
    "\n",
    "This notebook demonstrates access to [ECCOv4](https://ecco-group.org/) model output. Broad information about the ECCO dataset can be found in the PODAAC website (see [here](https://podaac.jpl.nasa.gov/cloud-datasets?view=list&ids=Projects&values=ECCO)).\n",
    "\n",
    "**Requirements to run this notebook**\n",
    "1. Have an Earth Data Login account\n",
    "2. Have a Bearer Token.\n",
    "\n",
    "**Objectives**\n",
    " \n",
    "Use [pydap](https://pydap.github.io/pydap/), Xarray and OPeNDAP to\n",
    "\n",
    "- Discover all OPeNDAP URLs associated with a ECCOv4 data collection. This is a **Level 4** dataset defined in the CubedSphere [ECCOv4](https://podaac.jpl.nasa.gov/cloud-datasets?view=list&ids=Projects&values=ECCO)\n",
    "- Authenticate via EDL (token based)\n",
    "- Explore ECCOv4 collection and filter variables, and coordinates\n",
    "- Consolidate Metadata at the collection level\n",
    "- Download/stream a subset of interest\n",
    "\n",
    "\n",
    "Some variables of focus are\n",
    "\n",
    "- [Temperature and Salinity](https://podaac.jpl.nasa.gov/dataset/ECCO_L4_TEMP_SALINITY_LLC0090GRID_MONTHLY_V4R4)\n",
    "\n",
    "\n",
    "`Author`: Miguel Jimenez-Urias, '25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69f13e-1576-46cc-88bf-8484bf53788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydap.net import create_session\n",
    "from pydap.client import get_cmr_urls, consolidate_metadata, open_url\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "import earthaccess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67fb4b-158e-4f25-84fe-cf35711bed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"xarray version: \", xr.__version__)\n",
    "print(\"pydap version: \", pydap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19f749-cd0f-499b-abe7-ffe8139b5ca0",
   "metadata": {},
   "source": [
    "## Explore the ECCOv4 Collection\n",
    "\n",
    "The entire ECCOv4 simulation is available via NASA's OPeNDAP service, but it is split into several collections. Each collection is available via Netcdf files. In this tutorial, we will subset Temperature, Salinity at the oceanic surface, for a region of interest: The North Atlantic Ocean. Subsetting the CubedSphere requires specialized code. Since the data is tiled, we will only focus for sake of simplicity, on subsetting the tiles.\n",
    "\n",
    "For more information about the Ocean Temperature and Salinity, you can checkout this resource https://podaac.jpl.nasa.gov/ECCO?sections=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592016ce-36cf-4298-b017-17fee64b5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecco_ts_ccid = \"C1991543728-POCLOUD\" # \n",
    "time_range = [dt.datetime(1992, 1, 1), dt.datetime(2017, 12, 31)] # One month of data\n",
    "\n",
    "cmr_urls = get_cmr_urls(ccid=ecco_ts_ccid, time_range=time_range, limit=1000) # you can incread the limit of results\n",
    "print(\"################################################ \\n We found a total of \", len(cmr_urls), \"OPeNDAP URLS!!!\\n################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65130f-8837-41d9-8d70-5342f55d94c8",
   "metadata": {},
   "source": [
    "## Authenticate\n",
    "\n",
    "To hide the abstraction, we will use earthaccess to authenticate, and create a `CachedSession` to consolidate all metadata pre-download. This approach can result in speed up of 100x when aggregating all URLs into a single Xarray Dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9da3a9-19f9-428d-a592-be71d85c700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy=\"interactive\", persist=True) # you will be promted to add your EDL credentials\n",
    "\n",
    "# pass Token Authorization to a new Session.\n",
    "cache_kwargs={'cache_name':'data/ECCOv4'}\n",
    "my_session = create_session(use_cache=True, session=auth.get_session(), cache_kwargs=cache_kwargs)\n",
    "my_session.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0042cb-053d-424b-971c-ed8b0d325725",
   "metadata": {},
   "source": [
    "## Explore Variables in collection and filter down to keep only desirable ones\n",
    "\n",
    "Here we demonstrate pydap as a exploratory metadata tool. Without downloading any array data, will let us know:\n",
    "\n",
    "- Variables Names, sizes, attributes\n",
    "- Dimensions and Coordinates\n",
    "- Allow us to construct an OPeNDAP Constraint Expression that subsets by variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced1396-be08-4a0b-b487-c430781bd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyds = open_url(cmr_urls[0].replace(\"https\", \"dap4\"), session=my_session)\n",
    "pyds.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71e926-602f-4e56-a92f-2a89dbaf8bd4",
   "metadata": {},
   "source": [
    "```{note}\n",
    "NetCDF files are self describing, and often contain all information necessary to interprete the data. At the collection level (dataset), much of this information is duplicated across each file. With OPeNDAP we can reduce the amount of data processed/download with `Constraint Expressions`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeafa57-cc97-4c6d-99f1-138ba7d5b432",
   "metadata": {},
   "source": [
    "We are interested only in Salinity (`SALT`) and Temperature (`THETA`), and the necessary extra dimensions/coordinates to work with this arrays. We use `PyDAP` to help us figure this out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a987764-bd3c-4385-b52c-8bcbd41038f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyds['THETA'].dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233c438-c3a3-49e5-87db-d97b514833ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyds['SALT'].dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a25bb-ff77-4173-b764-c53590d29953",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyds['THETA'].coordinates, pyds[\"SALT\"].coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b42d50-b476-4bce-8fe3-1fef68fb0d34",
   "metadata": {},
   "source": [
    "### Filter by Variable Names (CEs)\n",
    "\n",
    "\n",
    "We use the information on dimensions and coordinates, to further filter all possible variables in the dataset. We accomplish this by adding a query parameter of the form:\n",
    "\n",
    "```\n",
    "<base_url> + ?dap4.ce=/var_name1;...\n",
    "```\n",
    "Any variable name included in the query expression, will be processed and all others will be ignored.\n",
    "\n",
    "```{note}\n",
    "You can also construct the full URL with constraint expressions interactively, by selecting manually the variables on the datasets's [Data Request Form](https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090.dmr) and selecting **Copy (Raw) Data URL**. To go to this page, you need to append a `.dmr` to each opendap url and insert it into a browser.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb76860-48ba-4f9d-ba3b-7a21170840f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = pyds['THETA'].dims\n",
    "Vars = ['/THETA', \"/SALT\"] + dims\n",
    "\n",
    "# Below construct Contraint Expression\n",
    "CE = \"?dap4.ce=\"+(\";\").join(Vars)\n",
    "print(\"Constraint Expression: \", CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e53ba0-9842-4c9b-a48f-ab4c2c1b8763",
   "metadata": {},
   "source": [
    "### DAP4 URLs\n",
    "\n",
    "To tell pydap and Xarray which OPeNDAP protocol to use to stream data, we can replace the scheme of each url with `dap4`. DAP4 is a relatively new protocol (compared to DAP2, and it is widely used across NASA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8967a8-aa69-413d-aba1-9d97c2ffa4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECCO_urls = [url.replace(\"https\", \"dap4\") + CE for url in cmr_urls]\n",
    "ECCO_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c1737-dad3-40ce-9418-c62bbd2d42bc",
   "metadata": {},
   "source": [
    "### CubedSphere: Visualizing Depth in the native grid\n",
    "\n",
    "Before continuing, we explore the complex topology of the `ECCO` dataset via the Grid file.\n",
    "\n",
    "```{note}\n",
    "Many of the coordinate variables in the Temperature/Salinity file are also present in the Grid file.\n",
    "```\n",
    "\n",
    "`ECCO` data is defined on a Cube Sphere -- meaning the horizontal grid contains an `extra` dimension: `tile` or `face`. You can inspect the data in its native grid by plotting all horizontal data onto a grid. For that, we look at the `Depth` variable. This is NOT included in same collection as `Temperature`. Below we provide the Cloud OPeNDAP URL, which can be queried from the CMR or Earthdata Search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebb080-8cd1-41d1-b24c-7636ab9f4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concept collection ID for ECCO grid\n",
    "grid_ccid = \"C2013557893-POCLOUD\"\n",
    "Grid_url = get_cmr_urls(ccid=grid_ccid)[0] # only one element\n",
    "Grid_url = Grid_url.replace(\"https\", \"dap4\")\n",
    "\n",
    "grid_ds = open_url(Grid_url)\n",
    "grid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf9bd3-cc03-4c4a-81a3-2343b8e96d9c",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The coordinates for THETA and SALT are present in the single file for Grid. And so we will not include them in our workflow until the end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5bd5d7-3a7f-48bc-b7fd-65e7ad93d288",
   "metadata": {},
   "source": [
    "### Download a single variable\n",
    "\n",
    "With pure `PyDAP`, slicing an array triggers the download of that array. We illustrate that below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714cb04-4dbd-4b1e-9272-c553bfd5d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Depth = grid_ds[\"Depth\"][:].data  # <-------- LOADS DATA as in-memory numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f04da-b7ce-442e-b285-7f611b9b0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb837f-baa9-40a2-ba05-226bdff9ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable = [Depth[i] for i in range(13)]\n",
    "clevels =  np.linspace(0, 6000, 100)\n",
    "cMap = 'Greys_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd406c4b-39aa-4a5d-9fe4-9d769fd455c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(8, 8), gridspec_kw={'hspace':0.01, 'wspace':0.01})\n",
    "AXES = [\n",
    "    axes[4, 0], axes[3, 0], axes[2, 0], axes[4, 1], axes[3, 1], axes[2, 1],\n",
    "    axes[1, 1], \n",
    "    axes[1, 2], axes[1, 3], axes[1, 4], axes[0, 2], axes[0, 3], axes[0, 4],\n",
    "]\n",
    "for i in range(len(AXES)):\n",
    "    AXES[i].contourf(Variable[i], clevels, cmap=cMap)\n",
    "\n",
    "for ax in np.ravel(axes):\n",
    "    ax.axis('off')\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149ebc8-726b-45e6-bb7a-875902ef24de",
   "metadata": {},
   "source": [
    "**Fig. 1.** `Depth` plotted on a horizontal layout. Data on tiles `0-5` follow `C-ordering`, whereas data on tiles `7-13` follow `F-ordering`. Data on the `arctic cap`, is defined on a polar coordinate grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b48da6-c88a-40af-a8fb-e23b9d2bb3b0",
   "metadata": {},
   "source": [
    "**Plot with corrected Topology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9b1cf-76f9-457e-b1dc-0bcdabe19aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8), gridspec_kw={'hspace':0.01, 'wspace':0.01})\n",
    "AXES_NR = [\n",
    "    axes[3, 0], axes[2, 0], axes[1, 0], axes[3, 1], axes[2, 1], axes[1, 1],\n",
    "]\n",
    "AXES_CAP = [axes[0, 0]]\n",
    "AXES_R = [\n",
    "    axes[1, 2], axes[2, 2], axes[3, 2], axes[1, 3], axes[2, 3], axes[3, 3],\n",
    "]\n",
    "for i in range(len(AXES_NR)):\n",
    "    AXES_NR[i].contourf(Variable[i], clevels, cmap=cMap)\n",
    "\n",
    "for i in range(len(AXES_CAP)):\n",
    "    AXES_CAP[i].contourf(np.transpose(Variable[6])[:, ::-1], clevels, cmap=cMap)\n",
    "\n",
    "for i in range(len(AXES_R)):\n",
    "    AXES_R[i].contourf(np.transpose(Variable[7+i])[::-1, :], clevels, cmap=cMap)\n",
    "\n",
    "for ax in np.ravel(axes):\n",
    "    ax.axis('off')\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360c07e-7345-4781-8733-bb71c177a4dc",
   "metadata": {},
   "source": [
    "**Fig. 2.** `Depth` plotted on a horizontal layout that approximates `lat-lon` display. Data on the `arctic cap`, however, remains on a polar coordinate grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c89fce-81f2-4739-afc9-5f42c9b1b02b",
   "metadata": {},
   "source": [
    "\n",
    "## Consolidate Metadata\n",
    "\n",
    "Aggregating multiple remote files at the collection level requires persistent internet connection. The pydap backend allows to download and store the metadata required by xarray locally as a sqlite3 database, and this database can be used as session manager (for futher downloads). Creating this databaset can be done once, and reused, version controlled, etc. Reusing this database can cut the time to generate the aggregated dataset view from minutes to seconds. \n",
    "\n",
    "### Persisting metadata for later reuse\n",
    "\n",
    "When executing `consolidate_metadata` below, a sqlite3 database is created in the default directory specified by the cache_name, in the `CachedSession`. This metadata file can be stored persistently, and version-controlled, to avoid running consolidate metadata everytime. \n",
    "\n",
    "```{note}\n",
    "To persist the metadata database, avoid executing `session.cache.clear()`. You can also place the `.sqlite` in a version controlled repository local to your workflow that can track any changes to it. With this, re-executing `consolidate_metadata` will exit in `<1` second. \n",
    "```\n",
    "\n",
    "```{warning}\n",
    "After creatng the sqlite database, you can access it across Kernel restarts by simply `creating the session` as done above, choosing the `cache_name` that matches the name of the sqlite database. BUT you will may also need to re-execute `consolidate_metadata`, as in some cases `consolidate_metadata` creates a special key_fn to reuse the cache associated with certain dimensions across urls. This behavior will go away in a future release.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c77333-3adf-4e53-8974-1ef8d574f9e7",
   "metadata": {},
   "source": [
    "\n",
    "### The case of Coordinates\n",
    "\n",
    "The variables  `Z`, `YC`,and `XC` represent `ocean depth`, `latitude` and `longitude` respectively, in this curvilinear grid model. These are coordinates that appear in each of the remote file. In the presence of repeated coordinates, when aggregating into a single Dataset, `Xarray` will attempt to download each of them. This can result in 100s or requests downloading the same data over and over. `pydap.client.consolidat_metadata` reuse this coordinates, sparing 100s or download requests.\n",
    "\n",
    "While in this tutorial we are excluding this coordinates, to be able to include them one would have to define the additional argument in `consolidate_metadata`: `set_maps=True`. It is False by default. For example, if we were to keet the coordinates:\n",
    "\n",
    "```python\n",
    "consolidate_metadata(ECCO_urls, session=my_session, concat_dim='time', set_maps=True) \n",
    "```\n",
    "\n",
    "However **in this case, we can simply use the same coordinates present in the grid file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459b5f1-7614-45c5-aca5-68d46b355bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "consolidate_metadata(ECCO_urls, session=my_session, concat_dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa480b-4c1b-4eee-a7bf-9339c94d9553",
   "metadata": {},
   "source": [
    "## Dataset aggregation via Xarray\n",
    "\n",
    "Before creating the aggregated dataset, we know we are interested in:\n",
    "\n",
    "-  Surface data (a single element `k=0`, where `k` is the dimension).\n",
    "-  Tiles `2`, `6`, and `10`, cover the North Atlantic Ocean (see depth plots below).\n",
    "\n",
    "We want the OPeNDAP server to do the subsetting for us, reducing the amount of data downloaded. As such, we need to define the chunking that will match our server request. See below\n",
    "\n",
    "### A note about chunks\n",
    "\n",
    "With OPeNDAP, any download from a granule should be considered as a single chunk, from the perspective of Xarray. The DAP4 protocol does send chunk responses over the web, but these are assembled by pydap in memory as individual numpy arrays. You can see the perspective of Xarray on the remote chunking by passing a `chunks={}` argument when opening the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfba13b-1278-45a1-92f7-719787b24c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    ECCO_urls, \n",
    "    engine='pydap',\n",
    "    session=my_session, \n",
    "    parallel=True,\n",
    "    combine='nested',\n",
    "    concat_dim='time',\n",
    "    chunks={},\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12579b7-91a9-4c76-8af7-64a02a226419",
   "metadata": {},
   "source": [
    "### Xarray treats the remote data variable as an individual chunk, to matter the size of the OPeNDAP subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ecf38-f713-4e4e-9a3c-84fd948b69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['THETA'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87fea4-ec20-4320-9be9-2bdf6b601b88",
   "metadata": {},
   "source": [
    "### For Chunk Considerations\n",
    "\n",
    "\n",
    "Download performance is a game of `Chunk Sizes`. For OPeNDAP, this concerns the size of the slice, which is bounded from above by the size of the granule (at the variable level). However, if the chunks are too small, the download will take forever. Too large, and you will download too much data from S3 (assuming this tutorial is being run outside of the region). In that last scenario (data is on S3), egress charges are important, and we find ourselves with the different choices for our approach to download data:\n",
    "\n",
    "- Minimize the amount of data leaving the cloud. This is accompished by the following chunking: `chunks={'k':1, 'tile':1}`. However, since we will download 3 tiles per granule, chunking the dataset will multiply by a factor of 3 the amount of request made to the remote OPeNDAP server, only for the data to be assembled by Xarray afterwards, once downloaded locally and before storing.\n",
    "- Optimized performance. In this case, this is done by choosing the following chunking: `chunks={'k':1}`. It will download all the tiles per granule, and the top level only. All these are easily subsetted by Xarray once data is downloaded. This option also keeps the number of requests to the OPeNDAP server at a minimum. \n",
    "\n",
    "```{warning}\n",
    "Even with the optimized performance approach, this may not be fast. Download speed will depend on internet connection, or data locality. Overall, the number of request to the server will be `~600` in this scenario, all behind some form of authentication.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f8fec-9640-4c5a-8e6e-c7fe384fb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    ECCO_urls, \n",
    "    engine='pydap',\n",
    "    session=my_session,\n",
    "    parallel=True,\n",
    "    combine='nested',\n",
    "    concat_dim='time',\n",
    "    chunks={'k':1},\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef82522-6d03-4191-ab54-5f535d1fbd1c",
   "metadata": {},
   "source": [
    "### Include the grid data\n",
    "\n",
    "Now we will access the remote Grid file to include some additional grid variables. This new individual dataset will be merged with our temperature/salinity dataset.\n",
    "\n",
    "```{note}\n",
    "We will NOT use the same CachedSession object. Instead, we will initiate a different session object. Note the speed it takes may be similar (same order) as aggregating 100s of urls. This is what consolidate metadata does.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01539fa-9eb8-4e7c-989e-2e37d4b54510",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### create anew session\n",
    "session = create_session(session=auth.get_session())\n",
    "\n",
    "### create an individual dataset with only the variables of interest\n",
    "grid_ds = xr.open_dataset(Grid_url+\"?dap4.ce=/Depth;/XC;/YC;/i;/j;/tile\", engine='pydap', session=session)\n",
    "grid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fab96-f26d-4d07-9838-3c53da418074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combine the two datasets into a single dataset reference\n",
    "nds = xr.merge([ds, grid_ds])\n",
    "nds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168ef31-b6e7-48bb-addf-5d801e4b40ca",
   "metadata": {},
   "source": [
    "### Stream all surface data with OPeNDAP, ans subset tiles and store data locally with Xarray,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bea42d-aa45-42b0-b359-a4db3dcfc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nds.isel(k=0, tile=[2,6,10]).to_netcdf(\"data/ECCOv4_NA.nc4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef78a8c-fcac-4ba7-92df-0a1948464e1e",
   "metadata": {},
   "source": [
    "### Finally, inspect the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b43e2-1f94-4aea-be9f-c5ef75970f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = xr.open_dataset(\"data/ECCOv4_NA.nc4\")\n",
    "mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4d130-3f49-4aee-8452-1b9953953720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc98b7-929b-4817-8a5b-de87e9411b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable = [mds['THETA'][0, i, :, :] for i in range(3)]\n",
    "clevels = np.linspace(-5, 30, 100)\n",
    "cMap='RdBu_r'\n",
    "ocean_mask = mds[\"Depth\"]>0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501f6e7-53eb-4b74-88fe-dbd97f70bcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbc593-4a34-4a35-b411-1f320564b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8), gridspec_kw={'hspace':0.001, 'wspace':0.001})\n",
    "AXES_NR = [\n",
    "    axes[1, 1],\n",
    "]\n",
    "AXES_CAP = [axes[0, 1]]\n",
    "AXES_R = [\n",
    "    axes[1, 0],\n",
    "]\n",
    "for i in range(len(AXES_NR)):\n",
    "    ocean_mask.isel(tile=0).plot(ax=AXES_NR[i], cmap=\"Greys_r\", add_colorbar=False)\n",
    "    Variable[0].where(ocean_mask.isel(tile=0)).plot(ax=AXES_NR[i], levels=clevels, cmap=cMap, add_colorbar=False)\n",
    "\n",
    "for i in range(len(AXES_CAP)):\n",
    "    ocean_mask.isel(tile=1).transpose().plot(ax= AXES_CAP[i], cmap=\"Greys_r\", add_colorbar=False, xincrease=False)\n",
    "    Variable[1].transpose().where(ocean_mask.isel(tile=1)).plot(ax=AXES_CAP[i], levels=clevels, cmap=cMap, add_colorbar=False, xincrease=False)\n",
    "\n",
    "\n",
    "for i in range(len(AXES_R)):\n",
    "    # AXES_R[i].contourf(Variable[2].transpose()[::-1, :], clevels, cmap=cMap)\n",
    "    ocean_mask.isel(tile=2).transpose().plot(ax= AXES_R[i], cmap=\"Greys_r\", add_colorbar=False, yincrease=False)\n",
    "    Variable[2].transpose().where(ocean_mask.isel(tile=2)).plot(ax=AXES_R[i], levels=clevels, cmap=cMap, add_colorbar=False, yincrease=False)\n",
    "for ax in np.ravel(axes):\n",
    "    ax.axis('off')\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax.title, visible=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a83c6b-8e48-43b4-9b9e-f3d3896d9809",
   "metadata": {},
   "source": [
    "**Fig. 3.** `Surface temperature`, plotted on a horizontal layout that approximates `lat-lon` display. Data on the `arctic cap`, however, remains on a polar coordinate grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec5f83-600f-48fc-968c-f44447f1a839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
