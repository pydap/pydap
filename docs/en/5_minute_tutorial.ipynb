{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdc24e9-f0de-4820-871e-cea5bdbb9a73",
   "metadata": {},
   "source": [
    "# 5 Minute Tutorial\n",
    "\n",
    "## OPeNDAP - the vision\n",
    "The original vision of [OPeNDAP](https://www.opendap.org/) ([Cornillion, et al 1993](https://zenodo.org/records/10610992)) was to democratize remote data access, by making the equivalencies\n",
    "\n",
    "$ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\boxed{\\text{URL} \\approx \\text{Remote Dataset} }$\n",
    "\n",
    "$ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\boxed{\\text{URL + Constraints} \\approx \\text{Subset of Remote Dataset}} $\n",
    "\n",
    "That led to the development of the `DAP2` protocol (formerly known as `DODS`). Currently, <span style='color:#ff6666'>**OPeNDAP**</span> and Unidata servers implement the modern and broader  <span style='color:#0066cc'>**DAP4**</span> protocol (see [DAP4 specification](https://opendap.github.io/dap4-specification/DAP4.html#_how_dap4_differs_from_dap2)), to continue enabling the original vision of OPeNDAP.\n",
    "\n",
    "## What pydap enables:\n",
    "\n",
    "The internal logic of `PyDAP` enables the construction of constraint expressions for each url, interactively, hiding the abstraction away from the user. Furthermore, using `PyDAP` as a [backend engine](https://docs.xarray.dev/en/stable/user-guide/io.html#opendap) for `Xarray`, the original <span style='color:#ff6666'>**OPeNDAP**</span> vision can scaled with multi-core parallelism. Nonetheless, basic understanding about the use of Constraint Expression comes in handy when aggregating multiple files, and can lead to more efficient worklows.\n",
    "\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "\n",
    "- Demonstrate how to specify the <span style='color:#0066cc'>**DAP4**</span> protocol to the remote server.\n",
    "- Use `Xarray` with `PyDAP` as the backend engine to download a subset of remote data in two user case scenarios: `a)` an `NcML` aggregation file (virtual dataset), and `b)` across two Netcdf files.\n",
    "- Demonstrate distinct ways to use Constraint Expression (`CE`s), and how these are passed down to the remote server so that <span style='color:#0066cc'>**subsetting is done by the server**</span>, in a `data-proximate` way,  without performace loss on the client side.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Datasets behind a <span style='color:#0066cc'>**DAP4**</span> implementing server. For example, the test server: http://test.opendap.org/opendap/. \n",
    "- pydap>=3.5.8\n",
    "- xarray>=2025.0\n",
    "- numpy>=2.0\n",
    "\n",
    "```{note}\n",
    "The vast majority of NASA's OPeNDAP servers implement the DAP4 protocol.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db3dfb-0d2d-4bab-8b50-864bfc7602dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydap.client import open_url, consolidate_metadata, create_session\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50c2be-d275-4bcd-a056-e4f3fb615e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session to inspect downloads. cache_name must have `debug`\n",
    "session = create_session(use_cache=True, cache_kwargs={\"cache_name\":'debug_case1'})\n",
    "session.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b757b8-b7b6-455f-a3dd-de21e0a776a6",
   "metadata": {},
   "source": [
    "## Case 1) Subsetting an NcML file\n",
    "\n",
    "The file is an NcML file representing a virtually aggregated dataset, which can be found in the test server and it is named: [aggExisting.ncml](http://test.opendap.org/opendap/data/ncml/agg/aggExisting.ncml.dmr.html).\n",
    "\n",
    "<span style='color:#ff6666'>**OPeNDAP**</span> servers can be configured to produce NcML virtual datasets. Their advantage is that with an individual <span style='color:#ff6666'>**OPeNDAP**</span> url, a user has access to an entire collection of files from which to subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e6360-9c42-4ee7-8af6-3df567ae70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncml_url = \"http://test.opendap.org/opendap/data/ncml/agg/aggExisting.ncml\"\n",
    "dap4_ncml_url = ncml_url.replace(\"http\",  \"dap4\")\n",
    "print(\"=============================================================\\n Remote DAP4 URL: \\n\", dap4_ncml_url, \"\\n=============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76d28e-7b1e-4390-999c-6397b169c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\n",
    "    dap4_ncml_url, \n",
    "    engine='pydap',\n",
    "    session = session,\n",
    "    chunks={},\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a5e44-957c-481a-a950-76d8338a3097",
   "metadata": {},
   "source": [
    "### What happens if we download a single data point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ce8fc-d936-4749-a3ff-e076c2a5e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['T']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f1eae-dbc7-4ee8-872a-a209666ee7e3",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The info about chunking in `T` implies the entire array is treated as a single chunk! This is a stardard interpretation that `Xarray` makes of `OPeNDAP` urls. What happens if I download a subset of the data? \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72184e94-7e56-4028-b737-b23f6d674164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache to inspect what is being downloaded\n",
    "session.cache.clear() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33924d0b-a2ce-4ce5-a85e-c9f42290f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds['T'].isel(time=1, lon=0).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48720cc8-ce67-449d-9578-f6578794ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================== \\n Request sent to the Remote Server:\\n \", session.cache.urls()[0].split(\"?\")[-1].split(\"&dap4.checksum\")[0].replace(\"%5B\",\"[\").replace(\"%5D\",\"]\").replace(\"%3A\",\":\").replace(\"%2F\",\"/\"), \"\\n====================================== \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef4e30-9d94-4d93-a303-6ec4718e574a",
   "metadata": {},
   "source": [
    "<span style='color:#0066cc'>**The constraint expression is built from the**<span style='color:black'>\n",
    "`.isel` `Xarray` <span style='color:#0066cc'>**method and correctly passed to the server, which does all the subsetting work!**<span style='color:black'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697eb49-ffdd-45b3-9825-f4b8b28fa7d3",
   "metadata": {},
   "source": [
    "## Case 2) Subsetting across two separate files.\n",
    "\n",
    "The two files can be found in the test server, named: [coads_climatology](http://test.opendap.org/opendap/data/nc/coads_climatology.nc.dmr.html) and [coads_climatology2](http://test.opendap.org/opendap/data/nc/coads_climatology.nc.dmr.html). These two datasets share identical spatial dimensions, can be aggregated in time, and share almost all identical variables.\n",
    "\n",
    "```{note}\n",
    "It is important to always check that datasets can be aggregated. `PyDAP` and `Xarray` have internal logic to check if any two or more datasets can be concatenated. But all these safety checks only take into account dimensions and cooordinates.\n",
    "```\n",
    "\n",
    "<span style='color:#0066cc'>**An important step will be the use of Constraint Expressions (CEs) to ensure that only the  variables of interest are concatenating**<span style='color:black'>.\n",
    "\n",
    "```{warning}\n",
    "One of these files has extra variables not present in the other file, and that we will discarded by the use of CEs.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedc2d6-23de-42af-a6b4-5789244e8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"http://test.opendap.org/opendap/data/nc/coads_climatology.nc\", \"http://test.opendap.org/opendap/data/nc/coads_climatology2.nc\"]\n",
    "dap4_urls = [url.replace(\"http\",\"dap4\") for url in urls]\n",
    "\n",
    "# constraint expression\n",
    "dap4_CE = \"?dap4.ce=\" + \";\".join([\"/SST\", \"/COADSX\", \"/COADSY\", \"/TIME\"])\n",
    "\n",
    "# Final list of OPeNDAP URLs\n",
    "dap4ce_urls =[url+dap4_CE for url in dap4_urls]\n",
    "print(\"====================================================\\nThe following are the DAP4 OPeNDAP URLs \\n\", dap4ce_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f6d2d-5a60-4e37-aa93-4b46e2529a01",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**Q: Why use `CE`s when `Xarray` has a `.drop_variables` method?** Because `Xarray` needs to first parse the entirely of the remote metadata first, to subsequently drop the variables. In some files, there could be 1000 variables. `Xarray` would parse all these, and them drop them. With the `CE`, the server sends a Constrained Metadata associated with only the desired variables.\n",
    "```\n",
    "\n",
    "\n",
    "```{warning}\n",
    "`Xarray` expects the presence of dimension in the metadata. When constructing `CE`s, the user needs to make sure to include all the dimensions associated with the variables of interest in the CE. In the example above, `COASX`, `COADSY`, and `TIME` are the dimensions of `SST`.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ed693-77ab-4777-8b4a-b11a18ea0e69",
   "metadata": {},
   "source": [
    "### <span style='color:#0066cc'>**Consolidate Metadata speeds up the Dataset generation**<span style='color:black'>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f21d0-7ddb-4fb0-a222-afebcbb31ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate_metadata(dap4ce_urls, session=session, concat_dim=\"TIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0d52d-7c64-4689-be0f-a8caf70e0cb4",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`consolidate_metadata(dap4_urls, concat_dim='...', session=session)` downloads the dimensions of the remote file and stores them as a SQLite, to be reused. The session object becomes a way to authenticate, and act as a database manager! This practice can result in a performance gain of ~ 10-100 times faster workflows!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12184166-a4d7-4d80-86fe-e0838c4c13c0",
   "metadata": {},
   "source": [
    "### Use Xarray logic to download data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ef5ba-b1d4-4459-9f22-5b5dd027412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\n",
    "    dap4ce_urls, \n",
    "    engine='pydap',\n",
    "    concat_dim='TIME',\n",
    "    session=session,\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    "    decode_times=False,\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ecc7c2-c907-4d6f-b4ea-90a8f5a38e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['SST']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba5656-bdcf-4e03-84e0-f28c777064dc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The chunking of `SST` implies the entire array within each file is a single chunk! This is a stardard interpretation that `Xarray` makes of `OPeNDAP` urls. What if we download a single spatial point from a single remote file? \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414053a8-a9ef-4c98-8d0e-23d974bc430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfa280-b88f-4c53-816e-3ef912cb78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds['SST'].isel(TIME=0, COADSX=0, COADSY=0).load() # this should download a single point one of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89aa6d0-0ce7-4c6d-9c57-104d52e510b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================== \\n Request sent to the Remote Server:\\n \", session.cache.urls()[0].split(\"?\")[-1].split(\"&dap4.checksum\")[0].replace(\"%5B\",\"[\").replace(\"%5D\",\"]\").replace(\"%3A\",\":\").replace(\"%2F\",\"/\"), \"\\n====================================== \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904266d8-9879-4820-977d-df57127e9c17",
   "metadata": {},
   "source": [
    "### <span style='color:#0066cc'>**The entire variable is unnecessarily downloaded<span style='color:black'>** !!\n",
    "\n",
    "Ideally we would want the see the following Request (in the constraint expressssion) sent to the Remote Server:\n",
    "\n",
    "```python\n",
    "dap4.ce=/SST[0][0][0]\n",
    "```\n",
    "It seems that `xr.open_mfdataset` does not pass the slice argument to the server for each remote dataset. Instead it downloads all the chunk (i.e. the data array) in a single request, subsets it, and then aggregates the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342eca8-65b2-4a2b-a67e-84b4807dc7bc",
   "metadata": {},
   "source": [
    "### <span style='color:#0066cc'>**How to pass the slice from Xarray to the Remote Server<span style='color:black'>**\n",
    "\n",
    "\n",
    "**The answer is to `chunk` the dataset when creating it**. The chunk **should match the expected size of your subset**. That way the subset will be processed within a single request per remote file.\n",
    "\n",
    "```{warning}\n",
    "If you chunk the dataset with a size smaller that your expected download, you will trigger many downloads per remote file, forcing `Xarray` extra work to assemble the data together.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd373db5-e924-4144-8d4b-bd0766e0aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate metadata again, since the cached metadata was cleared before\n",
    "consolidate_metadata(dap4ce_urls, session=session, concat_dim=\"TIME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168b04a-444f-48cd-9b00-35e4189fe920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single element in all dimensions, the expected size of the download is:\n",
    "expected_sizes = {\"TIME\":1, \"COADSX\":1, \"COADSY\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0851c1-db8b-492b-8b4d-d84ce8342ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    dap4ce_urls, \n",
    "    engine='pydap',\n",
    "    concat_dim='TIME',\n",
    "    session=session,\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    "    decode_times=False,\n",
    "    chunks=expected_sizes,\n",
    ")\n",
    "session.cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94c77e-1231-4e25-83d0-7025f9156de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['SST'] # inspect chunks before download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18783b1-f637-48ce-b3ec-6505d6ebc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds['SST'].isel(TIME=0, COADSX=0, COADSY=0).load() # triggers download of an individual chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df444a09-b572-4a0c-ad83-0d90b68ee25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================== \\n Request sent to the Remote Server:\\n \", session.cache.urls()[0].split(\"?\")[-1].split(\"&dap4.checksum\")[0].replace(\"%5B\",\"[\").replace(\"%5D\",\"]\").replace(\"%3A\",\":\").replace(\"%2F\",\"/\"), \"\\n====================================== \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ca435-ee8f-44d7-b52f-b57c9142c2c6",
   "metadata": {},
   "source": [
    "### Warning: Be cautious about chunking\n",
    "\n",
    "We now only downloaded exactly what we requested! However, in some scenarios the time for download can be 10x slower, compared to the case when we requested more data!! The reason for the slowdown can sometimes be attributed to the number of chunks the dask graph generated.\n",
    "\n",
    "\n",
    "* `No chunking. Download all the array in the file. 2 chunks in 5 dask graphs (one per file).`\n",
    "* `Chunking. Download only the desired element of a file. 388800 chunks in 5 dask graphs`. \n",
    "\n",
    "Ideally, the chunk manager should only trigger the download of a single chunk. However, `388800` were created to ensure passing the slice to the server. This, can sometimes lead to slowdowns on the client side.\n",
    "\n",
    "In the scenario above, we went to the extremes. It is better to find a chunk compromise. We demonstrate that below, but <span style='color:#0066cc'>**now subsetting across all time (across both files)**</span>. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6b080-799d-49bf-a3ae-05f519e72fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate_metadata(dap4ce_urls, session=session, concat_dim=\"TIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4425e89-df96-46fa-9117-5d3a9ffa8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_sizes = {\"COADSY\":1} # note that we will subset across all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf4b0a-6ac2-4e69-8d5d-25ed5f453092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    dap4ce_urls, \n",
    "    engine='pydap',\n",
    "    concat_dim='TIME',\n",
    "    session=session,\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    "    decode_times=False,\n",
    "    chunks=download_sizes,\n",
    ")\n",
    "session.cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eae400-2227-4681-968e-35959f930584",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['SST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5cf1b7-5f15-4b86-ba10-4fd3a163e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds['SST'].isel(COADSX=0, COADSY=0).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fe1ad-078a-4a76-b5f4-dca23b57c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================== \\n Parallel Requests sent to the Remote Server:\\n \", [url.split(\"?\")[-1].split(\"&dap4.checksum\")[0].replace(\"%5B\",\"[\").replace(\"%5D\",\"]\").replace(\"%3A\",\":\").replace(\"%2F\",\"/\") for url in session.cache.urls()], \"\\n====================================== \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0752c6a-a804-41a1-b9d4-ef755a12c089",
   "metadata": {},
   "source": [
    "### Success! Similar timings but much and smaller download!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cca12b-d12a-4c45-a6d1-4f2256a73a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
